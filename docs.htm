<HTML>
<HEAD>
<TITLE>AI::NeuralNet::BackProp - A simple back-prop neural net that uses Delta's and Hebbs' rule.</TITLE>
<LINK REV="made" HREF="mailto:">
</HEAD>

<BODY>

<A NAME="__index__"></A>
<!-- INDEX BEGIN -->

<UL>

	<LI><A HREF="#name">NAME</A></LI>
	<LI><A HREF="#synopsis">SYNOPSIS</A></LI>
	<LI><A HREF="#description">DESCRIPTION</A></LI>
	<UL>

		<LI><A HREF="#methods">METHODS</A></LI>
	</UL>

	<LI><A HREF="#other included packages">OTHER INCLUDED PACKAGES</A></LI>
	<LI><A HREF="#notes">NOTES</A></LI>
	<UL>

		<LI><A HREF="#run() and slow_run() compared"><CODE>run()</CODE> and <CODE>slow_run()</CODE> compared</A></LI>
		<LI><A HREF="#load() and save()"><CODE>load()</CODE> and <CODE>save()</CODE></A></LI>
	</UL>

	<LI><A HREF="#bugs">BUGS</A></LI>
	<LI><A HREF="#author">AUTHOR</A></LI>
	<LI><A HREF="#download">DOWNLOAD</A></LI>
</UL>
<!-- INDEX END -->

<HR>
<P>
<H1><A NAME="name">NAME</A></H1>
<P>AI::NeuralNet::BackProp - A simple back-prop neural net that uses Delta's and Hebbs' rule.</P>
<P>
<HR>
<H1><A NAME="synopsis">SYNOPSIS</A></H1>
<PRE>
        use AI::NeuralNet::BackProp;
</PRE>
<PRE>

        # Create a new neural net with 2 layers and 3 neurons per layer
        my $net = new AI::NeuralNet::BackProp(2,3);</PRE>
<PRE>

        # Associate first pattern and print benchmark
        print &quot;Associating (1,2,3) with (4,5,6)...\n&quot;;
        print $net-&gt;learn([1,2,3],[4,5,6]);</PRE>
<PRE>

        # Associate second pattern and print benchmark
        print &quot;Associating (4,5,6) with (1,2,3)...\n&quot;;
        print $net-&gt;learn([4,5,6],[1,2,3]);</PRE>
<PRE>

        # Run a test pattern
        print &quot;\nFirst run output: (&quot;.join(',',@{$net-&gt;run([1,3,2])}).&quot;)\n\n&quot;;</PRE>
<PRE>

        # Declare patterns to learn
        my @pattern = ( 15, 3,  5  );
        my @result  = ( 16, 10, 11 );</PRE>
<PRE>

        # Display patterns to associate using sub interpolation into a string.
        print &quot;Associating (@{[join(',',@pattern)]}) with (@{[join(',',@result)]})...\n&quot;;</PRE>
<PRE>

        # Run learning loop and print benchmarking info.
        print $net-&gt;learn(\@pattern,\@result);</PRE>
<PRE>

        # Run final test
        my @test          = ( 14, 9,  3  );
        my $array_ref = $net-&gt;run(\@test);</PRE>
<PRE>

        # Display test output
        print &quot;\nSecond run output: (&quot;.join(',',@{$array_ref}).&quot;)\n&quot;;</PRE>
<P>
<HR>
<H1><A NAME="description">DESCRIPTION</A></H1>
<P>AI::NeuralNet::BackProp is the flagship package for this file.
It implements a nerual network similar to a feed-foward,
back-propagtion network; learning via a mix of a generalization
of the Delta rule and a disection of Hebbs rule. The actual 
neruons of the network are implemented via the AI::NeuralNet::BackProp::neuron package.
</P>
<PRE>

You constuct a new network via the new constructor:</PRE>
<PRE>

        my $net = new AI::NeuralNet::BackProp(2,3);</PRE>
<P>The <CODE>new()</CODE> constructor accepts two arguments, $layers and $size (in this example, $layers 
is 2 and $size is 3).</P>
<P>$layers specifies the number of layers, including the input
and the output layer, to use in each neural grouping. A new
neural grouping is created for each pattern learned. Layers
is typically set to 2. Each layer has $size neurons in it.
Each neuron's output is connected to one input of every neuron
in the layer below it. 
</P>
<PRE>

This diagram illustrates a simple network, created with a call
to &quot;new AI::NeuralNet::BackProp(2,2)&quot; (2 layers, 2 neurons/layer).</PRE>
<PRE>

         input
         /  \
        O    O
        |\  /|
        | \/ |
        | /\ |
        |/  \|
        O    O
         \  /
        mapper</PRE>
<P>In this diagram, each neuron is connected to one input of every
neuron in the layer below it, but there are not connections
between neurons in the same layer. Weights of the connection
are controlled by the neuron it is connected to, not the connecting
neuron. (E.g. the connecting neuron has no idea how much weight
its output has when it sends it, it just sends its output and the
weighting is taken care of by the receiving neuron.) This is the 
method used to connect cells in every network built by this package.</P>
<P>Input is fed into the network via a call like this:</P>
<PRE>
        use AI;
        my $net = new AI::NeuralNet::BackProp(2,2);
</PRE>
<PRE>

        my @map = (0,1);</PRE>
<PRE>

        my $result = $net-&gt;run(\@map);</PRE>
<P>Now, this call would probably not give what you want, because
the network hasn't ``learned'' any patterns yet. But this
illustrates the call. Run expects an array refrence, and 
run gets mad if you don't give it what it wants. So be nice.</P>
<P>Run returns a refrence with $size elements (Remember $size? $size
is what you passed as the second argument to the network
constructor.) This array contains the results of the mapping. If
you ran the example exactly as shown above, $result would contain
(1,1) as its elements.</P>
<P>To make the network learn a new pattern, you simply call the learn
method with a sample input and the desired result, both array
refrences of $size length. Example:</P>
<PRE>
        use AI;
        my $net = new AI::NeuralNet::BackProp(2,2);
</PRE>
<PRE>

        my @map = (0,1);
        my @res = (1,0);</PRE>
<PRE>

        $net-&gt;learn(\@map,\@res [, $inc]);</PRE>
<PRE>

        my $result = $net-&gt;run(\@map);</PRE>
<P>$inc is an optinal learning speed increment. Good values are around 0.20
and 0.30. You can experiement with $inc to achieve faster learning speeds.
Some values of $inc work better for different maps. If $inc is ommitted,
it will default to 0.30 for $inc internally.</P>
<P>Now $result will conain (1,0), effectivly flipping the input pattern
around. Obviously, the larger $size is, the longer it will take
to learn a pattern. <CODE>Learn()</CODE> returns a string in the form of</P>
<PRE>
        Learning took X loops and X wallclock seconds (X.XXX usr + X.XXX sys = X.XXX CPU).</PRE>
<P>With the X's replaced by time or loop values for that loop call. So,
to view the learning stats for every learn call, you can just:
</P>
<PRE>

        print $net-&gt;learn(\@map,\@res);</PRE>
<P>If you call ``$net-&gt;debug(4)'' with $net being the 
refrence returned by the <CODE>new()</CODE> constructor, you will get benchmarking 
information for the learn function, as well as plenty of other information output. 
See notes on <A HREF="#item_debug"><CODE>debug()</CODE></A> , in METHODS, below.</P>
<P>If you do call $net-&gt;debug(1), it is a good 
idea to point STDIO of your script to a file, as a lot of information is output. I often
use this command line:</P>
<PRE>
        $ perl some_script.pl &gt; .out</PRE>
<P>Then I can simply go and use emacs or any other text editor and read the output at my leisure,
rather than have to wait or use some 'more' as it comes by on the screen.</P>
<P>This system was originally created to be a type of content-addressable-memory
system. As such, it implements ``groups'' for storing patterns and
maps. After the network has learned the patterns you want, then you
can call run with a pattern it has never seen before, and it will
decide which of the stored patterns best fit the new pattern, returning
the results the same as the above examples (as an array ref from $net-&gt;run()).</P>
<P>
<H2><A NAME="methods">METHODS</A></H2>
<DL>
<DT><STRONG><A NAME="item_BackProp">new AI::NeuralNet::BackProp($layers, $size)</A></STRONG><BR>
<DD>
Returns a newly created neural network from an <CODE>AI::NeuralNet::BackProp</CODE>
object. Each group of this network will have <CODE>$layers</CODE> number layers in it
and each layer will have <CODE>$size</CODE> number of neurons in that layer.
<P>Before you can really do anything useful with your new neural network
object, you need to teach it some patterns. See the <A HREF="#item_learn"><CODE>learn()</CODE></A> method, below.</P>
<P></P>
<DT><STRONG><A NAME="item_learn">$net-&gt;learn($input_map_ref, $desired_result_ref [, $learning_gradient ]);</A></STRONG><BR>
<DD>
This will 'teach' a network to associate an new input map with a desired resuly.
It will return a string containg benchmarking information. You can retrieve the
pattern index that the network stored the new input map in after <A HREF="#item_learn"><CODE>learn()</CODE></A> is complete
with the <A HREF="#item_pattern"><CODE>pattern()</CODE></A> method, below.
<P>The first two arguments must be array refs, and must be of the same length.</P>
<P>$learning_gradient is an optional value used to adjust the weights of the internal
connections. If $learning_gradient is ommitted, it defaults to 0.30.</P>
<P></P>
<DT><STRONG><A NAME="item_run">$net-&gt;run($input_map_ref);</A></STRONG><BR>
<DD>
This compares the input map with the learn()ed input map of each group, and the 
group who's comparission comes out as the lowest percentage difference is 
then used to run the input map.
<P>It will return undef on an error. An error is caused by one of two events.</P>
<P>The first is the possibility that the argument passed is not an array ref. If it
is not an array ref, it returns silently a value of undef.</P>
<P>The other condition that could cause an error is the fact that your map contained an 
element with an undefined value. We don't allow this because it has been in testing that
an undefined value can never be weighted. We all know that 12958710924 times 0 is still
0, right? The network can't handle that, though. It will still try to apply as much weight
as it can to a 0 value, but the weighting will  always come back 0, and therefore, never be able
to match the desired result output, thereby creating an infinite <A HREF="#item_learn"><CODE>learn()</CODE></A> loop cycle. Soooo...
to prevent the infinite looping, we simply don't allow 0 values to be run. You can always
shift all values of your map up one number to account for this if need be, and then subtract
one number from every element of the output to shift it down again. Let me know if anyone 
comes up with a better way to do this.</P>
<P><A HREF="#item_run"><CODE>run()</CODE></A> will store the pattern index of the group as created by learn(), so it can be 
retrieved with the <A HREF="#item_pattern"><CODE>pattern()</CODE></A> method, below.</P>
<P>See notes on comparison between <A HREF="#item_run"><CODE>run()</CODE></A> and <A HREF="#item_slow_run"><CODE>slow_run()</CODE></A> in NOTES section, below.</P>
<P></P>
<DT><STRONG><A NAME="item_slow_run">$net-&gt;slow_run($input_map_ref);</A></STRONG><BR>
<DD>
When called with an array refrence to a pattern, returns a refrence
to an array associated with that pattern. See usage in documentatio above.
<P>This <A HREF="#item_slow_run"><CODE>slow_run()</CODE></A> is different from run(), above, in that, the <A HREF="#item_run"><CODE>run()</CODE></A> above
compares the input map with the learn()ed input map of each group, and the 
group who's comparission comes out as the lowest percentage difference is 
then used to run the input map.</P>
<P>This <A HREF="#item_slow_run"><CODE>slow_run()</CODE></A> runs the input map through every neuron group and then compares
the result <CODE>map()</CODE> with the learn()ed result map, and the result map that has the
lowest comparrison percentage is returned as the output map. Some may argue
that this could be more accurate. I don't know. I plan to run some more tests
on the two methods, but right now I don't have the time. If anyone does 
come up with any results, or even a better way to sort the outputs, let me know
please (<A HREF="mailto:jdb@wcoil.com">jdb@wcoil.com</A>)</P>
<P><A HREF="#item_slow_run"><CODE>slow_run()</CODE></A> will store the pattern index of the group as created by learn(), so it can be 
retrieved with the <A HREF="#item_pattern"><CODE>pattern()</CODE></A> method, below.</P>
<P>See notes on comparison between <A HREF="#item_run"><CODE>run()</CODE></A> and <A HREF="#item_slow_run"><CODE>slow_run()</CODE></A> in NOTES section, below.</P>
<P></P>
<DT><STRONG><A NAME="item_pattern">$net-&gt;pattern();</A></STRONG><BR>
<DD>
This will return the pattern index of the last map learned, or the pattern index of the
last map matched, whichever occured most recently.
<P>This is useful if you don't care about the mapping output, but just what it mapped.
For example, in the letters.pl example under the ./examples/ directory in the installation
tree that you should have gotten when you downloaded this pacakge, this method is used
to determine which letter was matched, rather than what the output is. See letters.pl
for example usage.</P>
<P></P>
<DT><STRONG><A NAME="item_benchmarked">$net-&gt;benchmarked();</A></STRONG><BR>
<DD>
This returns a benchmark info string for the last <A HREF="#item_learn"><CODE>learn()</CODE></A> or the last <A HREF="#item_run"><CODE>run()</CODE></A> call, 
whichever occured later. It is easily printed as a string,
as following:
<PRE>
        print $net-&gt;benchmarked() . &quot;\n&quot;;</PRE>
<P></P>
<DT><STRONG><A NAME="item_debug">$net-&gt;<CODE>debug($level)</CODE></A></STRONG><BR>
<DD>
Toggles debugging off if called with $level = 0 or no arguments. There are four levels
of debugging.
<P>Level 0 ($level = 0) : Default, no debugging information printed, except for the 'Cannot run
0 value.' error message. Other than that one message, all printing is left to calling script.</P>
<P>Level 1 ($level = 1) : This causes ALL debugging information for the network to be dumped
as the network runs. In this mode, it is a good idea to pipe your STDIO to a file, especially
for large programs.</P>
<P>Level 2 ($level = 2) : A slightly-less verbose form of debugging, not as many internal 
data dumps.</P>
<P>Level 3 ($level = 3) : JUST prints weight mapping as weights change.</P>
<P>Level 4 ($level = 4) : JUST prints the benchmark info for EACH learn loop iteteration, not just
learning as a whole. Also prints the percentage difference for each loop between current network
results and desired results, as well as learning gradient ('incremenet').</P>
<P>Level 4 is useful for seeing if you need to give a smaller learning incrememnt to <A HREF="#item_learn"><CODE>learn()</CODE></A> .
I used level 4 debugging quite often in creating the letters.pl example script and the small_1.pl
example script.</P>
<P>Toggles debuging off when called with no arguments.</P>
<P></P>
<DT><STRONG><A NAME="item_join_cols">$net-&gt;join_cols($array_ref,$row_length_in_elements,$high_state_character,$low_state_character);</A></STRONG><BR>
<DD>
This is more of a utility function than any real necessary function of the package.
Instead of joining all the elements of the array together in one long string, like <CODE>join()</CODE> ,
it prints the elements of $array_ref to STDIO, adding a newline (\n) after every $row_length_in_elements
number of elements has passed. Additionally, if you include a $high_state_character and a $low_state_character,
it will print the $high_state_character (can be more than one character) for every element that
has a true value, and the $low_state_character for every element that has a false value. 
If you do not supply a $high_state_character, or the $high_state_character is a null or empty or 
undefined string, it <A HREF="#item_join_cols"><CODE>join_cols()</CODE></A> will just print the numerical value of each element seperated
by a null character (\0). <A HREF="#item_join_cols"><CODE>join_cols()</CODE></A> defaults to the latter behaviour.
<P></P>
<DT><STRONG><A NAME="item_pdiff">$net-&gt;pdiff($array_ref_A, $array_ref_B);</A></STRONG><BR>
<DD>
This function is used VERY heavily internally to calculate the difference in percent
between elements of the two array refs passed. It returns a %.02f (sprintf-format) 
percent sting.
<P></P>
<DT><STRONG><A NAME="item_intr">$net-&gt;intr($float);</A></STRONG><BR>
<DD>
Rounds a floating-point number passed to an integer using <CODE>sprintf()</CODE> and <CODE>int()</CODE> , Provides
better rounding than just calling <CODE>int()</CODE> on the float. Also used very heavily internally.
<P></P></DL>
<P>
<HR>
<H1><A NAME="other included packages">OTHER INCLUDED PACKAGES</A></H1>
<DL>
<DT><STRONG><A NAME="item_AI%3A%3ANeuralNet%3A%3ABackProp%3A%3AFile">AI::NeuralNet::BackProp::File</A></STRONG><BR>
<DD>
<A HREF="#item_AI%3A%3ANeuralNet%3A%3ABackProp%3A%3AFile"><CODE>AI::NeuralNet::BackProp::File</CODE></A> implements a simple 'relational'-style
database system. It is used internally by <CODE>AI::NeuralNet::BackProp</CODE> for 
storage and retrival of network states. It can also be used independently
of <CODE>AI::NeuralNet::BackProp</CODE>. PODs are not yet included for this package, I hope
to include documentation for this package in future releases.
<P></P>
<DT><STRONG><A NAME="item_AI%3A%3ANeuralNet%3A%3ABackProp%3A%3Aneuron">AI::NeuralNet::BackProp::neuron</A></STRONG><BR>
<DD>
AI::NeuralNet::BackProp::neuron is the worker package for AI::NeuralNet::BackProp.
It implements the actual neurons of the nerual network.
AI::NeuralNet::BackProp::neuron is not designed to be created directly, as
it is used internally by AI::NeuralNet::BackProp.
<P></P>
<DT><STRONG><A NAME="item_AI%3A%3ANeuralNet%3A%3ABackProp%3A%3A_run">AI::NeuralNet::BackProp::_run</A></STRONG><BR>
<DD>
<DT><STRONG><A NAME="item_AI%3A%3ANeuralNet%3A%3ABackProp%3A%3A_map">AI::NeuralNet::BackProp::_map</A></STRONG><BR>
<DD>
These two packages, _run and _map are used to insert data into
the network and used to get data from the network. The _run and _map packages 
are connected to the neurons so that the neurons think that the IO packages are
just another neuron, sending data on. But the IO packs. are special packages designed
with the same methods as neurons, just meant for specific IO purposes. You will
never need to call any of the IO packs. directly. Instead, they are called whenever
you use the <A HREF="#item_run"><CODE>run()</CODE></A> or <A HREF="#item_learn"><CODE>learn()</CODE></A> methods of your network.
<P></P></DL>
<P>
<HR>
<H1><A NAME="notes">NOTES</A></H1>
<P>
<H2><A NAME="run() and slow_run() compared"><A HREF="#item_run"><CODE>run()</CODE></A> and <A HREF="#item_slow_run"><CODE>slow_run()</CODE></A> compared</A></H2>
<P>Authors thoughts...</P>
<P>Hmm.. this could be something. I just realized:
With <A HREF="#item_slow_run"><CODE>slow_run()</CODE></A> , it compares the _outputs_
(result of each group running input map) with
the desired result (output) map of each group. 
<A HREF="#item_run"><CODE>run()</CODE></A> only compares the input map with the <A HREF="#item_learn"><CODE>learn()</CODE></A> ed
input map.</P>
<P>With <A HREF="#item_run"><CODE>run()</CODE></A> , that means that the first group that matches
closest with <A HREF="#item_learn"><CODE>learn()</CODE></A> ed maps will be run, even if you
have learned the same map with different desired results.</P>
<P>With <A HREF="#item_slow_run"><CODE>slow_run()</CODE></A> , you could conceivably learn one input
map with multiple desired results, and then <A HREF="#item_slow_run"><CODE>slow_run()</CODE></A> will
match the result from the input map against all desired results,
and return the one that matches closest.</P>
<P>Intersting idea. For now, I don't see much of a need for 
multiple desired asociations with same input map.</P>
<P>Please let me know if anyone sees any other pros or cons
to this issue, and what you think should be done.</P>
<P>
<H2><A NAME="load() and save()"><CODE>load()</CODE> and <CODE>save()</CODE></A></H2>
<P>These are two methods I have not documented, as they don't 
work (correctly) yet. They rely on the Storable package, not
included, and the AI::NeuralNet::BackProp::File pacakge, 
included here.</P>
<P>The AI::NeuralNet::BackProp::File package works fine, the
problem lies in the <CODE>load()</CODE> and <CODE>save()</CODE> routines themselves. 
It seems the <CODE>freeze()</CODE> and <CODE>thaw()</CODE> functions aren't handling
the refrences very well.</P>
<P>I included these functions in this beta release in case anyone
felt dareing enough to try to get them working themselves. If you
do, <EM>please</EM> send me a copy of the code! :-)</P>
<P>
<HR>
<H1><A NAME="bugs">BUGS</A></H1>
<P>This is the beta release of <CODE>AI::NeuralNet::BackProp</CODE>, and that holding true, I am sure 
there are probably bugs in here which I just have not found yet. If you find bugs in this module, I would 
appreciate it greatly if you could report them to me at <EM>&lt;<A HREF="mailto:jdb@wcoil.com">jdb@wcoil.com</A>&gt;</EM>,
or, even better, try to patch them yourself and figure out why the bug is being buggy, and
send me the patched code, again at <EM>&lt;<A HREF="mailto:jdb@wcoil.com">jdb@wcoil.com</A>&gt;</EM>.</P>
<P>
<HR>
<H1><A NAME="author">AUTHOR</A></H1>
<P>Josiah Bryan <EM>&lt;<A HREF="mailto:jdb@wcoil.com">jdb@wcoil.com</A>&gt;</EM></P>
<P>Copyright (c) 2000 Josiah Bryan. All rights reserved. This program is free software; 
you can redistribute it and/or modify it under the same terms as Perl itself.</P>
<P>The <CODE>AI::NeuralNet::BackProp</CODE> and related modules are free software. THEY COMES WITHOUT WARRANTY OF ANY KIND.</P>
<P>
<HR>
<H1><A NAME="download">DOWNLOAD</A></H1>
<P>You can always download the latest copy of AI::NeuralNet::BackProp
from <A HREF="http://www.josiah.countystart.com/modules/AI/cgi-bin/rec.pl">http://www.josiah.countystart.com/modules/AI/cgi-bin/rec.pl</A></P>

</BODY>

</HTML>
